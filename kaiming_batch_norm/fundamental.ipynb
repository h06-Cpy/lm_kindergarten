{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step epoch iteration\n",
    "\n",
    "| 용어 | 의미 |\n",
    "|---|---|\n",
    "| Epoch | 전체 학습 데이터셋을 한 번 모두 모델에 학습시키는 주기입니다. |\n",
    "| Iteration | 한 번의 미니 배치(batch)를 모델에 학습시키는 단위입니다. |\n",
    "| Step | 일반적으로 iteration과 동일한 의미로 사용됩니다. (특히 PyTorch에서) |\n",
    "\n",
    "\n",
    "전체 데이터셋에 샘플이 1,000개 있다고 가정하겠습니다.\n",
    "\n",
    "한 번에 학습에 사용하는 배치 크기(batch_size)는 100개라고 하겠습니다.\n",
    "\n",
    "이 경우,\n",
    "\n",
    "1 Epoch = 전체 데이터 1,000개를 다 학습   \n",
    "→ 100개씩 10번 반복해야 하므로   \n",
    "→ 10 Iterations (or 10 Steps)\n",
    "\n",
    "---\n",
    "\n",
    "다음 식은 참고로만 알아두죠!\n",
    "\n",
    "$\\Large step = \\frac{N\\times \\text{epoch}}{\\text{\\# of GPU} \\times \\text{batch size} \\times \\text{gradient accumulation}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "> 모델이 훈련 데이터에만 **과하게 적합(overfitting) 되는 것을 막기 위해**, 모델의 복잡도를 제한하는 방법\n",
    "\n",
    "흔히 시련이 있으면 더 성장한다고 하죠   \n",
    "학습할 때 모델에 시련을 줘서 더 성능이 좋아지도록 합니다 (특히 테스트 데이터에서 성능)\n",
    "\n",
    "| 종류                | 설명                                                     |\n",
    "| ----------------- | ------------------------------------------------------ |\n",
    "| **L1 규제 (Lasso)** | 가중치들의 절댓값 합에 패널티를 줍니다. → **희소한 모델(많은 가중치를 0으로)** 만듭니다. |\n",
    "| **L2 규제 (Ridge)** | 가중치들의 제곱합에 패널티를 줍니다. → **큰 가중치를 작게 줄이는 역할**을 합니다.      |\n",
    "| **Dropout**       | 학습 시 일부 뉴런을 무작위로 꺼서 특정 뉴런에 의존하지 않게 합니다.                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMA(Exponential Moving Average, 지수 이동 평균)\n",
    "> 지수 이동 평균(EMA) 은 최근 값을 더 중요하게 반영하며, 과거 값을 점점 덜 반영하는 가중 평균\n",
    "\n",
    "이번에 배울 배치 정규화(batch normalization)에서 running mean, running var를 추정할 때 나옵니다!   \n",
    "말 그대로 추정값이라 실제 값과 오차가 있을 수 있습니다.\n",
    "\n",
    "$\\Large \\mu_t = \\alpha x_t + (1-\\alpha)\\mu_{t-1}$\n",
    "\n",
    "- $x_t$: 현재 배치에서 측정된 통계값 (예: 배치 평균, 분산 등)\n",
    "- $\\mu_{t-1}$: 이전까지의 EMA\n",
    "- $\\alpha$: decay 계수 (0~1 사이)\n",
    "\n",
    "torch 예제를 봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2896, -0.6921, -1.4356,  ..., -0.9725, -1.2570,  1.5115])\n",
      "평균: 0.0021140272729098797\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "num_samples: int = 10000\n",
    "\n",
    "# 전체 데이터 생성\n",
    "data: torch.Tensor = torch.randn(num_samples)\n",
    "\n",
    "print(data)\n",
    "\n",
    "# 전체 데이터 기준 정답 통계\n",
    "true_mean = data.mean()\n",
    "\n",
    "# 출력\n",
    "print(f\"평균: {true_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMA: 0.0026559513062238693\n"
     ]
    }
   ],
   "source": [
    "# EMA 파라미터 초기화\n",
    "ema = 0\n",
    "momentum = 0.001 \n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "# 배치 단위로 EMA 업데이트\n",
    "for i in range(0, num_samples, batch_size):\n",
    "    mini_batch = data[i:i+batch_size]\n",
    "    ema = momentum * ema + (1 - momentum) * mini_batch.mean()\n",
    "\n",
    "print(f\"EMA: {ema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "차이가 있지만 평균이랑 EMA랑 비슷하네요!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
